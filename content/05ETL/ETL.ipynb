{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole ETL process is conducted through a Python script connecting both the MongoDB and the PostgreSQL databases using **pymongo** and **sqlachemy** respectively. The resulting data is finally stored in the Warehouse. This is a repetitive script as well, executing every second day at **10:05AM** .This is achieved with the following cron command:\n",
    "```\n",
    "5 10 */2 * * python3 path_to_script/script.py\n",
    "```\n",
    "\n",
    "Click the toggle button to see the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy import Table,Column, Integer, String, Date, MetaData\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "import os\n",
    "os.chdir(os.path.dirname(__file__))\n",
    "start = datetime.now()\n",
    "countries_dict = {'it': 'Italy','de':'Germany','fr':'France','cz':'Czech Republic','tr':'Turkey','at':'Austria','lu':'Luxemburg',\n",
    "                  'be':'Belgium','nl':'Netherlands','gb':'Great Britain','es':'Spain','bg':'Bulgaria','ru':'Russia','pl':'Poland',\n",
    "                  'ro':'Romania','hu':'Hungary'}\n",
    "\n",
    "DATABASE_URI = 'postgres+psycopg2://<name>:<pass>@<IP_address>/<db_name>'\n",
    "meta = MetaData() \n",
    "engine = create_engine(DATABASE_URI)\n",
    "\n",
    "client = MongoClient('mongodb+srv://<User>:<Password>@dwprojectcluster.lpqbf.mongodb.net/cars_database?retryWrites=true&w=majority')\n",
    "\n",
    "df_cars = pd.DataFrame(list(client.cars_database.cars.find({})))\n",
    "df_cars.drop('_id', axis = 1, inplace = True)\n",
    "df_cars = df_cars[df_cars['Loaded_in_DW'].eq(False)]\n",
    "df_cars['Make'] = df_cars['Make'].replace({np.nan: 'Undefined'})\n",
    "df_cars['Model'] = df_cars['Model'].replace({np.nan: 'Undefined'})\n",
    "\n",
    "df_ads = pd.DataFrame(list(client.cars_database.ads.find({})))\n",
    "df_ads.drop(['_id', 'Price'], axis =1, inplace = True)\n",
    "\n",
    "df_sellers = pd.DataFrame(list(client.cars_database.sellers.find({})))\n",
    "df_sellers.drop('_id', axis = 1, inplace = True)\n",
    "df_sellers['City'] = df_sellers['City'].str.lower()\n",
    "\n",
    "df_ads.drop_duplicates(subset=['CarID'], inplace = True)\n",
    "df_cars.drop_duplicates(subset=['ID'], inplace = True)\n",
    "df_sellers.drop_duplicates(subset = ['SellerID'], inplace = True)\n",
    "print(\"Stignav\")\n",
    "df_cities = pd.read_csv('cities.csv')\n",
    "print(\"I posle gradovi\")\n",
    "df_cities_eu = df_cities.loc[df_cities['Country'].eq('de') | df_cities['Country'].eq('it')\n",
    "                             | df_cities['Country'].eq('at') | df_cities['Country'].eq('nl')\n",
    "                             | df_cities['Country'].eq('be') | df_cities['Country'].eq('lu')\n",
    "                             | df_cities['Country'].eq('pl') | df_cities['Country'].eq('ru')\n",
    "                             | df_cities['Country'].eq('es') | df_cities['Country'].eq('fr')\n",
    "                             | df_cities['Country'].eq('cz') | df_cities['Country'].eq('ro')\n",
    "                             | df_cities['Country'].eq('gb') | df_cities['Country'].eq('cz')\n",
    "                             | df_cities['Country'].eq('hu') | df_cities['Country'].eq('tr')\n",
    "                             | df_cities['Country'].eq('bg')]\n",
    "\n",
    "df_cities_eu.drop_duplicates(subset=['City'], inplace = True)\n",
    "\n",
    "sellers_extended = df_sellers.join(df_cities_eu.set_index('City'), on = 'City',rsuffix='_other')\n",
    "sellers_extended.loc[sellers_extended['Country'].isnull(), 'Country'] = sellers_extended['Country_other'].map(countries_dict)\n",
    "sellers_extended['Country'] = sellers_extended['Country'].replace({np.nan: 'Undefined'})\n",
    "\n",
    "\n",
    "join1 = df_cars.merge(df_ads, left_on = 'ID', right_on = 'CarID', how = 'inner')\n",
    "join2 = join1.merge(sellers_extended, left_on = 'SellerID', right_on = 'SellerID', how = 'inner')\n",
    "final = join2[['Make','Model','SellerID','Country','Mileage','Price']]\n",
    "final.isnull().sum()\n",
    "\n",
    "# remove rows which do not have both Make and Model \n",
    "final = final[~(final['Make'].isna() | final['Model'].isna())]\n",
    "# these are the removed documents (rows of the dataframe)\n",
    "documents_without_make_and_model = final[~(final['Make'].isna() | final['Model'].isna())]\n",
    "\n",
    "# performing imputation on a different dataframe\n",
    "imputation_dataframe = final.copy()\n",
    "# SellerID can't contribute to the imputation\n",
    "imputation_dataframe.drop('SellerID', axis = 1, inplace = True)\n",
    "# one-hot encoding the categorical variables, so KNN can be used for imputing\n",
    "imputation_dataframe = pd.get_dummies(data = imputation_dataframe,\n",
    "                                      columns = ['Country', 'Make', 'Model'],\n",
    "                                      dummy_na = False)\n",
    "\n",
    "# can be done without a pipeline, but in case of more steps in the future, the structure remains a pipeline\n",
    "imputation_pipeline = Pipeline(steps = [('iterative_imputer', KNNImputer(missing_values = np.nan,\n",
    "                                                                         n_neighbors = 5,\n",
    "                                                                         weights = 'distance',\n",
    "                                                                         metric = 'nan_euclidean',\n",
    "                                                                         copy = False,\n",
    "                                                                         add_indicator = False))])\n",
    "# applying pipeline and creating resulting dataframe\n",
    "imputed_dataframe = pd.DataFrame(data = imputation_pipeline.fit_transform(imputation_dataframe),\n",
    "                                 columns = imputation_dataframe.columns,\n",
    "                                 dtype = int)\n",
    "\n",
    "# adding the imputed columns to the original dataframe\n",
    "final.drop(['Mileage', 'Price'], axis = 1, inplace = True)\n",
    "final['Mileage'] = imputed_dataframe['Mileage']\n",
    "final['Price'] = imputed_dataframe['Price']\n",
    "\n",
    "# replacing null values with appropriate value defined by the team\n",
    "final['Country'] = final['Country'].replace({np.nan: 'Undefined'})\n",
    "final['Make'] = final['Make'].replace({np.nan: 'Undefined'})\n",
    "final['Model'] = final['Model'].replace({np.nan: 'Undefined'})\n",
    "\n",
    "model_postgre = pd.read_sql_table('model', con = engine)\n",
    "marka_postgre = pd.read_sql_table('marka', con = engine)\n",
    "seller_postgre = pd.read_sql_table('seller', con = engine)\n",
    "zemja_postgre = pd.read_sql_table('zemja', con = engine)\n",
    "\n",
    "left_join = pd.DataFrame({'make':list(set(df_cars['Make']))}).merge(marka_postgre, left_on = 'make', right_on = 'ime', how = 'left')\n",
    "left_join = left_join[left_join['skey'].isnull()]\n",
    "left_join['ime'] = left_join['make']\n",
    "left_join.drop(['skey', 'make'], axis =1, inplace = True)\n",
    "left_join.to_sql('marka', con=engine, if_exists='append', index=False)\n",
    "marka_postgre = pd.read_sql_table('marka', con = engine)\n",
    "\n",
    "left_join = pd.DataFrame({'model':list(set(df_cars['Model']))}).merge(model_postgre, left_on = 'model', right_on = 'ime', how = 'left')\n",
    "left_join = left_join[left_join['skey'].isnull()]\n",
    "left_join['ime'] = left_join['model']\n",
    "left_join.drop(['skey', 'model'], axis =1, inplace = True)\n",
    "left_join.to_sql('model', con=engine, if_exists='append', index=False,  method = 'multi')\n",
    "model_postgre = pd.read_sql_table('model', con = engine)\n",
    "\n",
    "\n",
    "left_join = pd.DataFrame({'zemja':list(set(sellers_extended['Country']))}).merge(zemja_postgre, left_on = 'zemja', right_on = 'ime', how = 'left')\n",
    "left_join = left_join[left_join['skey'].isnull()]\n",
    "left_join['ime'] = left_join['zemja']\n",
    "left_join.drop(['skey', 'zemja'], axis =1, inplace = True)\n",
    "left_join.to_sql('zemja', con=engine, if_exists='append', index=False,  method = 'multi')\n",
    "zemja_postgre = pd.read_sql_table('zemja', con = engine)\n",
    "\n",
    "left_join = pd.DataFrame({'zemja':list(set(sellers_extended['Country']))}).merge(zemja_postgre, left_on = 'zemja', right_on = 'ime', how = 'left')\n",
    "left_join = left_join[left_join['skey'].isnull()]\n",
    "left_join['ime'] = left_join['zemja']\n",
    "left_join.drop(['skey', 'zemja'], axis =1, inplace = True)\n",
    "left_join.to_sql('zemja', con=engine, if_exists='append', index=False, method = 'multi')\n",
    "zemja_postgre = pd.read_sql_table('zemja', con = engine)\n",
    "\n",
    "\n",
    "left_join = pd.concat([df_sellers['SellerID'], df_sellers['Vendor']], axis=1).merge(seller_postgre, left_on = 'SellerID', right_on = 'odb_sellerid', how = 'left')\n",
    "left_join = left_join[left_join['skey'].isnull()]\n",
    "left_join['odb_sellerid'] = left_join['SellerID']\n",
    "left_join['vendorname'] = left_join['Vendor']\n",
    "left_join.drop(['skey', 'SellerID', 'Vendor'], axis =1, inplace = True)\n",
    "left_join.to_sql('seller', con=engine, if_exists='append', index=False, method = 'multi')\n",
    "seller_postgre = pd.read_sql_table('seller', con = engine)\n",
    "\n",
    "\n",
    "merged = final.merge(seller_postgre, left_on = 'SellerID', right_on = 'odb_sellerid', how = 'inner') \\\n",
    "     .merge(zemja_postgre, left_on ='Country', right_on = 'ime', how = 'inner')  \\\n",
    "     .merge(marka_postgre, left_on = 'Make', right_on = 'ime', how = 'inner') \\\n",
    "     .merge(model_postgre, left_on = 'Model', right_on = 'ime', how = 'inner')[['skey_x', 'skey_y', 'Price', 'Mileage']]\n",
    "merged.columns = ['seller_skey', 'marka_skey', 'zemja_skey', 'model_skey','Price','Mileage']\n",
    "\n",
    "\n",
    "fact1 = merged.drop('seller_skey', axis =1)\n",
    "fact2 = merged.drop(['zemja_skey','model_skey'], axis =1)\n",
    "\n",
    "fact1 = fact1.groupby(['marka_skey', 'zemja_skey','model_skey']).agg(\n",
    "    avg_price = ('Price','mean'),\n",
    "    car_count = ('Price', 'count'),\n",
    "    avg_mileage = ('Mileage','mean')\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "\n",
    "fact2 =  fact2.groupby(['seller_skey', 'marka_skey']).agg(\n",
    "    car_count = ('Price', 'count'),\n",
    "\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "engine.execute(\"TRUNCATE TABLE fact_make_model_country\")\n",
    "engine.execute(\"TRUNCATE TABLE fact_seller_make\")\n",
    "\n",
    "fact1.to_sql('fact_make_model_country', con=engine, if_exists='append', index=False, method = 'multi')\n",
    "fact2.to_sql('fact_seller_make', con=engine, if_exists='append', index=False, method = 'multi')\n",
    "\n",
    "\n",
    "finish = datetime.now()\n",
    "log = {'Start': [start], 'Finish': [finish]}\n",
    "df_log = pd.DataFrame(data=log)\n",
    "df_log.to_sql('log_table', con=engine, if_exists='append', index=False, method = 'multi')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
